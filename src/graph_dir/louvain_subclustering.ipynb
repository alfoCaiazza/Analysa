{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sknetwork as skn\n",
    "from sknetwork.clustering import Louvain, get_modularity\n",
    "from sknetwork.linalg import normalize\n",
    "from sknetwork.utils import get_membership\n",
    "from sknetwork.visualization import visualize_graph\n",
    "import networkx as nx  \n",
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, to_networkx\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import SVG\n",
    "from cdlib import evaluation, NodeClustering, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34411056",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = os.path.join('..','..', 'src', 'data', 'graph_data.pt')\n",
    "data = torch.load(graph_dir, weights_only=False)\n",
    "graph = data['data']\n",
    "mapping = data['idx2user']\n",
    "\n",
    "# Extracting nodes position\n",
    "G_nx = to_networkx(graph, to_undirected=True)\n",
    "communities = algorithms.louvain(G_nx, weight='weight', resolution=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusterize community in cluster i\n",
    "exp_clusters = {}\n",
    "\n",
    "for k, cluster in enumerate(communities.communities):\n",
    "    users = [mapping[node_id] for node_id in cluster]\n",
    "    exp_clusters[k] = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subgraph(cluster_id, users, edge_df, save_path):\n",
    "    print(f\"Processing cluster : {cluster_id} with {len(users)} users.\")\n",
    "    users = set(users)\n",
    "    edges = edge_df[edge_df['source'].isin(users) & edge_df['target'].isin(users)]\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        return\n",
    "    \n",
    "    # Ensure consistent ordering of users\n",
    "    users = sorted(users)\n",
    "\n",
    "    # Mapping id - users \n",
    "    user2idx = {uid: idx for idx, uid in enumerate(users)}\n",
    "    idx2user = {idx: uid for uid, idx in user2idx.items()}\n",
    "\n",
    "    # Creating edges and associated weights\n",
    "    edges_index = torch.tensor([\n",
    "        [user2idx[src] for src in edges['source']],\n",
    "        [user2idx[dst] for dst in edges['target']],\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    edges_weight = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
    "\n",
    "    # Creating the subgraph aka the k-th cluster\n",
    "    x = torch.eye(len(user2idx))\n",
    "    data = Data(x=x, edge_index=edges_index, edge_weight=edges_weight)\n",
    "    torch.save({\n",
    "        'data': data,\n",
    "        'idx2user': idx2user,\n",
    "    }, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_with_Louvain(edge_df, subgraph_path, save_dir, parent_dir=None, threshold=0.2, min_users=20):\n",
    "    subgraph = torch.load(subgraph_path, weights_only=False)\n",
    "    data = subgraph['data']\n",
    "    idx2user = subgraph['idx2user']\n",
    "    \n",
    "    # Conversion to nx object\n",
    "    SUB_nx = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Calculating communities using Louvain\n",
    "    communities = algorithms.louvain(SUB_nx, weight='weight', resolution=1)\n",
    "    counductances = evaluation.conductance(SUB_nx,communities, summary=False)\n",
    "\n",
    "    cluster_nodes = {}\n",
    "\n",
    "    for i, (cluster, score) in enumerate(zip(communities.communities, counductances)):\n",
    "        users = [idx2user[node] for node in cluster]\n",
    "\n",
    "        if score < threshold and len(cluster) >= min_users:\n",
    "            # Build and save subgraph for this cluster\n",
    "            # Get user ids from node indices (integers)\n",
    "            true_save_dir = parent_dir if parent_dir else save_dir\n",
    "            save_path = os.path.join(true_save_dir, f\"subgraph_{i}.pt\")\n",
    "            build_subgraph(cluster_id=i, users=users, edge_df=edge_df, save_path=save_path)\n",
    "            print(f\"Added cluster {i} to list. Conductance: {score:.4f}, Users: {len(cluster)}\")\n",
    "        elif score >= threshold and len(cluster) >= min_users:\n",
    "            cluster_nodes[i] = cluster\n",
    "            print(f\"Keeping cluster {i} for further clusterization. Conductance: {score:.4f}, Users: {len(cluster)}\")\n",
    "        else:\n",
    "            print(f\"Cluster {i} discarded due to conductance and/or size. Conductance: {score:.4f}, Users: {len(cluster)}\")\n",
    "\n",
    "    return cluster_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853691ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_clustering(edge_df, subgraph_path, base_cluster_id, base_output_dir, cluster_tree, depth=0, max_depth = 4):\n",
    "    # Stopping condition\n",
    "    if depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    subgraph = torch.load(subgraph_path, weights_only=False)\n",
    "    idx2user = subgraph['idx2user']\n",
    "    \n",
    "    # Save current cluster info in tree\n",
    "    cluster_tree[int(base_cluster_id)] = {\n",
    "        \"users\" : {str(idx2user[i]) for i in range(len(idx2user))},\n",
    "        \"children\" : {}\n",
    "    }\n",
    "    child_tree = cluster_tree[int(base_cluster_id)][\"children\"]\n",
    "\n",
    "    # Use a dedicated directory for storing sub-subgraphs of this cluster\n",
    "    cluster_save_dir = os.path.join(base_output_dir, f\"subgraph_{base_cluster_id}\")\n",
    "    os.makedirs(cluster_save_dir, exist_ok=True)\n",
    "    \n",
    "    new_clusters= get_cluster_with_Louvain(edge_df, subgraph_path, save_dir=cluster_save_dir, parent_dir=base_cluster_id)\n",
    "    if not new_clusters:\n",
    "        return\n",
    "\n",
    "    for sub_id, node_ids in new_clusters.items():\n",
    "        users = [idx2user[idx] for idx in node_ids]\n",
    "\n",
    "        cluster_id = f\"{base_cluster_id}_{sub_id}\"\n",
    "        save_path = os.path.join(cluster_save_dir, f\"subgraph_{cluster_id}.pt\")\n",
    "        \n",
    "        new_path = build_subgraph(cluster_id, users, edge_df, save_path=save_path)\n",
    "        if new_path:\n",
    "            recursive_clustering(\n",
    "            edge_df=edge_df,\n",
    "            subgraph_path=new_path,\n",
    "            base_cluster_id=cluster_id,\n",
    "            base_output_dir=cluster_save_dir,\n",
    "            cluster_tree = child_tree,\n",
    "            depth=depth + 1,\n",
    "            max_depth=max_depth\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb75316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to start the recursion\n",
    "def start_recursive_clustering(first_clusters, edges_path, base_output_dir, max_depth=4):\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    cluster_tree = {}\n",
    "\n",
    "    for k, users in first_clusters.items():\n",
    "        save_path = os.path.join(base_output_dir, f'subgraph_{k}.pt')\n",
    "        new_cluster_path = build_subgraph(k, users, edges, save_path)\n",
    "        if new_cluster_path is not None:\n",
    "            recursive_clustering(\n",
    "                subgraph_path=new_cluster_path,\n",
    "                base_cluster_id=k,\n",
    "                edge_df=edges,\n",
    "                base_output_dir=base_output_dir,\n",
    "                cluster_tree=cluster_tree,\n",
    "                depth=0,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "\n",
    "    return cluster_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): make_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_json_serializable(v) for v in obj]\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb64c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_path = os.path.join('..', '..', 'src', 'data', 'edges.csv')\n",
    "save_dir = os.path.join('..', '..', 'src', 'graph_dir', 'subgraph_dir')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "cluster_tree = start_recursive_clustering(exp_clusters, edges_path, save_dir)\n",
    "json_safe_tree = make_json_serializable(cluster_tree)\n",
    "\n",
    "with open(os.path.join(save_dir, 'cluster_tree.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_safe_tree, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".analysa (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
