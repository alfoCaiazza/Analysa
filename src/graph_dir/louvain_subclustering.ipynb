{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sknetwork as skn\n",
    "from sknetwork.clustering import Louvain, get_modularity\n",
    "from sknetwork.linalg import normalize\n",
    "from sknetwork.utils import get_membership\n",
    "from sknetwork.visualization import visualize_graph\n",
    "import networkx as nx  \n",
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, to_networkx\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import SVG\n",
    "from cdlib import evaluation, NodeClustering, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subgraph(cluster_id, users, edge_df, save_path):\n",
    "    print(f\"Processing cluster : {cluster_id} with {len(users)} users.\")\n",
    "    users = set(users)\n",
    "    edges = edge_df[edge_df['source'].isin(users) & edge_df['target'].isin(users)]\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        return\n",
    "    \n",
    "    # Ensure consistent ordering of users\n",
    "    users = sorted(users)\n",
    "\n",
    "    # Mapping id - users \n",
    "    user2idx = {uid: idx for idx, uid in enumerate(users)}\n",
    "    idx2user = {idx: uid for uid, idx in user2idx.items()}\n",
    "\n",
    "    # Creating edges and associated weights\n",
    "    edges_index = torch.tensor([\n",
    "        [user2idx[src] for src in edges['source']],\n",
    "        [user2idx[dst] for dst in edges['target']],\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    edges_weight = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
    "\n",
    "    # Creating the subgraph aka the k-th cluster\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    x = torch.eye(len(user2idx))\n",
    "    data = Data(x=x, edge_index=edges_index, edge_weight=edges_weight)\n",
    "    torch.save({\n",
    "        'data': data,\n",
    "        'idx2user': idx2user,\n",
    "    }, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_with_Louvain(edge_df, subgraph_path, save_dir, threshold=0.2, min_users=20, base_cluster_id=None):\n",
    "    subgraph = torch.load(subgraph_path, weights_only=False)\n",
    "    data = subgraph['data']\n",
    "    idx2user = subgraph['idx2user']\n",
    "    \n",
    "    # Conversion to nx object\n",
    "    SUB_nx = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Calculating communities using Louvain\n",
    "    communities = algorithms.louvain(SUB_nx, weight='weight', resolution=1)\n",
    "    conductances = evaluation.conductance(SUB_nx,communities, summary=False)\n",
    "\n",
    "    cluster_nodes = {}\n",
    "\n",
    "    for i, (cluster, score) in enumerate(zip(communities.communities, conductances)):\n",
    "        users = [idx2user[node] for node in cluster]\n",
    "\n",
    "        full_id = f\"{base_cluster_id}_{i}\" if base_cluster_id is not None else str(i)\n",
    "        save_path = os.path.join(save_dir, f\"subgraph_{full_id}.pt\")\n",
    "\n",
    "        if score < threshold and len(cluster) >= min_users:\n",
    "            build_subgraph(full_id, users, edge_df, save_path=save_path)\n",
    "            print(f\"Added cluster {i} to list. Conductance: {score:.4f}, Users: {len(cluster)}\")\n",
    "        elif score >= threshold and len(cluster) >= min_users:\n",
    "            cluster_nodes[i] = cluster\n",
    "            print(f\"Keeping cluster {i} for further clusterization. Conductance: {score:.4f}, Users: {len(cluster)}\")\n",
    "        else:\n",
    "            print(f\"Cluster {i} discarded due to conductance and/or size. Conductance: {score:.4f}, Users: {len(cluster)}\")\n",
    "\n",
    "    return cluster_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853691ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_clustering(edge_df, subgraph_path, base_cluster_id, base_output_dir, cluster_tree, depth=0, max_depth = 3):\n",
    "    # Stopping condition\n",
    "    if depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    subgraph = torch.load(subgraph_path, weights_only=False)\n",
    "    idx2user = subgraph['idx2user']\n",
    "    \n",
    "    # Save current cluster info in tree\n",
    "    cluster_tree[str(base_cluster_id)] = {\n",
    "        \"users\" : [str(idx2user[i]) for i in range(len(idx2user))],\n",
    "        \"children\" : {}\n",
    "    }\n",
    "    child_tree = cluster_tree[str(base_cluster_id)][\"children\"]\n",
    "    \n",
    "    new_clusters= get_cluster_with_Louvain(\n",
    "        edge_df,\n",
    "        subgraph_path,\n",
    "        save_dir=base_output_dir,\n",
    "        base_cluster_id=base_cluster_id\n",
    "    )\n",
    "\n",
    "    if not new_clusters: #No more cluster to explore\n",
    "        return\n",
    "\n",
    "    for sub_id, node_ids in new_clusters.items():\n",
    "        users = [idx2user[idx] for idx in node_ids]\n",
    "        cluster_id = f\"{base_cluster_id}_{sub_id}\"\n",
    "\n",
    "        # Saving all .pt in the same directory\n",
    "        save_path = os.path.join(base_output_dir, f\"subgraph_{cluster_id}.pt\")\n",
    "        new_path = build_subgraph(cluster_id, users, edge_df, save_path=save_path)\n",
    "\n",
    "        if new_path:\n",
    "            recursive_clustering(\n",
    "            edge_df=edge_df,\n",
    "            subgraph_path=new_path,\n",
    "            base_cluster_id=cluster_id,\n",
    "            base_output_dir=base_output_dir,\n",
    "            cluster_tree = child_tree,\n",
    "            depth=depth + 1,\n",
    "            max_depth=max_depth\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb75316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to start the recursion\n",
    "def start_recursive_clustering(first_clusters, edges_path, base_output_dir, max_depth=3):\n",
    "    edges = pd.read_csv(edges_path)\n",
    "    cluster_tree = {}\n",
    "\n",
    "    for k, users in first_clusters.items():\n",
    "        cluster_dir = os.path.join(base_output_dir, f'cluster_{k}')\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(cluster_dir, f'subgraph_{k}.pt')\n",
    "        new_cluster_path = build_subgraph(k, users, edges, save_path)\n",
    "        \n",
    "        if new_cluster_path is not None:\n",
    "            recursive_clustering(\n",
    "                subgraph_path=new_cluster_path,\n",
    "                base_cluster_id=k,\n",
    "                edge_df=edges,\n",
    "                base_output_dir=cluster_dir,\n",
    "                cluster_tree=cluster_tree,\n",
    "                depth=0,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "\n",
    "    return cluster_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34411056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dirt cluster from the global graph\n",
    "edges_path = os.path.join('..', '..', 'src', 'data', 'edges.csv')\n",
    "edges_df = pd.read_csv(edges_path)\n",
    "\n",
    "save_dir = os.path.join('..', '..', 'src', 'graph_dir', 'louvain_subgraph_dir')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "json_cluster_tree_dir = os.path.join('..', '..', 'src', 'graph_dir',)\n",
    "graph_dir = os.path.join('..','..', 'src', 'data', 'graph_data.pt')\n",
    "\n",
    "data = torch.load(graph_dir, weights_only=False)\n",
    "graph = data['data']\n",
    "mapping = data['idx2user']\n",
    "\n",
    "# Extracting nodes position\n",
    "G_nx = to_networkx(graph, to_undirected=True)\n",
    "communities = algorithms.louvain(G_nx, weight='weight', resolution=1)\n",
    "len(communities.communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conductances = evaluation.conductance(G_nx, communities, summary=False)\n",
    "conductances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expliciting users for each fouded cluster and evaluating theri conducance\n",
    "exp_clusters = {}\n",
    "cluster_tree_base = {}\n",
    "conductance_threshold = 0.25\n",
    "min_users = 10\n",
    "\n",
    "for k, (cluster, score) in enumerate(zip(communities.communities, conductances)):\n",
    "    # If the cluster meets the criteria -> saves it directly\n",
    "    if score < conductance_threshold and len(cluster) >= min_users:\n",
    "        print(f\"Saving Cluster {k}: Conductance : {score} - Total Users : {len(cluster)}\")\n",
    "\n",
    "        cluster_tree_base[str(k)] = {\n",
    "        \"users\" : [str(mapping[i]) for i in range(len(mapping))]\n",
    "        }\n",
    "\n",
    "        cluster_dir = os.path.join(save_dir, f'cluster_{k}')\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "        cluster_path = os.path.join(cluster_dir, f'cluster_{k}.pt')\n",
    "\n",
    "        users = [mapping[node_id] for node_id in cluster]\n",
    "        build_subgraph(cluster_id=k, users=users, edge_df=edges_df, save_path=cluster_path)\n",
    "\n",
    "    # Else if the cluster overcome the maximum conductance threshold but has more then min_users users -> saves for futher clusterization\n",
    "    elif score > conductance_threshold and len(cluster) >= min_users:\n",
    "        print(f\"Collecting Cluster {k}: Conductance : {score} - Total Users : {len(cluster)}\")\n",
    "        users = [mapping[node_id] for node_id in cluster]\n",
    "        exp_clusters[k] = users\n",
    "    # Else if cluster doesnt meets any criteria -> removes nodes from the analysis -> noise\n",
    "    else:\n",
    "        print(f\"Deleting Cluster {k}: Conductance : {score} - Total Users : {len(cluster)}\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'cluster_tree_base.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(cluster_tree_base, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb64c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively creating sub-scluster form master cluster\n",
    "cluster_tree = start_recursive_clustering(exp_clusters, edges_path, save_dir)\n",
    "\n",
    "with open(os.path.join(json_cluster_tree_dir, 'cluster_tree.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(cluster_tree, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".analysa (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
