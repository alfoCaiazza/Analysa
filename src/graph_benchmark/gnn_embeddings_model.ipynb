{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f97d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.transforms import RandomLinkSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60bc5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, aggr='mean'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.SAGEConv(in_channels, 2 * out_channels, aggr=aggr) # Eventually change layer type\n",
    "        self.batch1 = pyg_nn.BatchNorm(2 * out_channels)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.conv2 = pyg_nn.SAGEConv(2 * out_channels, out_channels,  aggr=aggr) # Eventually change layer type\n",
    "        self.batch2 = pyg_nn.BatchNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index): # Modify to adapt to the dataset complexity\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.batch1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.batch2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        # By default, it computes the inner product of the node embeddings\n",
    "        adj = torch.matmul(z[edge_index[0]], z[edge_index[1]].t())\n",
    "        return adj[edge_index[0], edge_index[1]]\n",
    "    \n",
    "# Defining traing and evaluation functions\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_data.edge_index.to(device))\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    # Encode the node features to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_data.edge_index.to(device))\n",
    "    \n",
    "    # Compute the reconstruction loss\n",
    "    return model.test(z, pos_edge_index.to(device), neg_edge_index.to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb870c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "import os\n",
    "LATENT_DIM = 32\n",
    "data_path = os.path.join('..', 'data', 'graph_data.pt')\n",
    "dataset = torch.load(data_path, weights_only=False)\n",
    "data = dataset['data']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Instantiate the customized encoder in GAE model\n",
    "transform = RandomLinkSplit(is_undirected=True, split_labels=True,add_negative_train_samples=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "model = pyg_nn.GAE(Encoder(data.num_features, LATENT_DIM)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x = train_data.x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcbf51ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, AUC: 0.5122, AP: 0.4666\n",
      "Epoch: 020, AUC: 0.4856, AP: 0.4499\n",
      "Epoch: 030, AUC: 0.6260, AP: 0.5633\n",
      "Epoch: 040, AUC: 0.7563, AP: 0.6762\n",
      "Epoch: 050, AUC: 0.8088, AP: 0.7294\n",
      "Epoch: 060, AUC: 0.8189, AP: 0.7363\n",
      "Epoch: 070, AUC: 0.8245, AP: 0.7414\n",
      "Epoch: 080, AUC: 0.8255, AP: 0.7418\n",
      "Epoch: 090, AUC: 0.8146, AP: 0.7324\n",
      "Epoch: 100, AUC: 0.8187, AP: 0.7369\n",
      "Epoch: 110, AUC: 0.7470, AP: 0.6829\n",
      "Epoch: 120, AUC: 0.8014, AP: 0.7616\n",
      "Epoch: 130, AUC: 0.7782, AP: 0.7498\n",
      "Epoch: 140, AUC: 0.8467, AP: 0.8188\n",
      "Epoch: 150, AUC: 0.8514, AP: 0.8226\n",
      "Epoch: 160, AUC: 0.8471, AP: 0.8198\n",
      "Epoch: 170, AUC: 0.8380, AP: 0.8114\n",
      "Epoch: 180, AUC: 0.8512, AP: 0.8234\n",
      "Epoch: 190, AUC: 0.8573, AP: 0.8294\n",
      "Epoch: 200, AUC: 0.8387, AP: 0.8123\n",
      "Epoch: 210, AUC: 0.8605, AP: 0.8321\n",
      "Epoch: 220, AUC: 0.8563, AP: 0.8273\n",
      "Epoch: 230, AUC: 0.8604, AP: 0.8320\n",
      "Epoch: 240, AUC: 0.8529, AP: 0.8211\n",
      "Epoch: 250, AUC: 0.8539, AP: 0.8229\n",
      "Epoch: 260, AUC: 0.8495, AP: 0.8178\n",
      "Epoch: 270, AUC: 0.8462, AP: 0.8158\n",
      "Epoch: 280, AUC: 0.8636, AP: 0.8326\n",
      "Epoch: 290, AUC: 0.8608, AP: 0.8321\n",
      "Epoch: 300, AUC: 0.8582, AP: 0.8263\n",
      "Epoch: 310, AUC: 0.8495, AP: 0.8236\n",
      "Epoch: 320, AUC: 0.8634, AP: 0.8337\n",
      "Epoch: 330, AUC: 0.8617, AP: 0.8318\n",
      "Epoch: 340, AUC: 0.8666, AP: 0.8365\n",
      "Epoch: 350, AUC: 0.8451, AP: 0.8114\n",
      "Epoch: 360, AUC: 0.8588, AP: 0.8303\n",
      "Epoch: 370, AUC: 0.8415, AP: 0.8081\n",
      "Epoch: 380, AUC: 0.8288, AP: 0.7927\n",
      "Epoch: 390, AUC: 0.8454, AP: 0.8139\n",
      "Epoch: 400, AUC: 0.8434, AP: 0.8149\n",
      "Epoch: 410, AUC: 0.8229, AP: 0.7829\n",
      "Epoch: 420, AUC: 0.8565, AP: 0.8275\n",
      "Epoch: 430, AUC: 0.8392, AP: 0.8145\n",
      "Epoch: 440, AUC: 0.8533, AP: 0.8271\n",
      "Epoch: 450, AUC: 0.8365, AP: 0.8070\n",
      "Epoch: 460, AUC: 0.7303, AP: 0.6800\n",
      "Epoch: 470, AUC: 0.7604, AP: 0.7182\n",
      "Epoch: 480, AUC: 0.8376, AP: 0.8087\n",
      "Epoch: 490, AUC: 0.8286, AP: 0.7895\n",
      "Epoch: 500, AUC: 0.8486, AP: 0.8199\n",
      "Epoch: 510, AUC: 0.8480, AP: 0.8194\n",
      "Epoch: 520, AUC: 0.8248, AP: 0.7997\n",
      "Epoch: 530, AUC: 0.8382, AP: 0.8067\n",
      "Epoch: 540, AUC: 0.8253, AP: 0.8024\n",
      "Epoch: 550, AUC: 0.8473, AP: 0.8202\n",
      "Epoch: 560, AUC: 0.8600, AP: 0.8334\n",
      "Epoch: 570, AUC: 0.8187, AP: 0.7920\n",
      "Epoch: 580, AUC: 0.7777, AP: 0.7410\n",
      "Epoch: 590, AUC: 0.8461, AP: 0.8224\n",
      "Epoch: 600, AUC: 0.8371, AP: 0.8141\n",
      "Epoch: 610, AUC: 0.8530, AP: 0.8245\n",
      "Epoch: 620, AUC: 0.8546, AP: 0.8281\n",
      "Epoch: 630, AUC: 0.8408, AP: 0.8110\n",
      "Epoch: 640, AUC: 0.8531, AP: 0.8214\n",
      "Epoch: 650, AUC: 0.8182, AP: 0.7860\n",
      "Epoch: 660, AUC: 0.8245, AP: 0.7912\n",
      "Epoch: 670, AUC: 0.8495, AP: 0.8227\n",
      "Epoch: 680, AUC: 0.8447, AP: 0.8183\n",
      "Epoch: 690, AUC: 0.8484, AP: 0.8216\n",
      "Epoch: 700, AUC: 0.8468, AP: 0.8143\n",
      "Epoch: 710, AUC: 0.8499, AP: 0.8258\n",
      "Epoch: 720, AUC: 0.8422, AP: 0.8060\n",
      "Epoch: 730, AUC: 0.8531, AP: 0.8228\n",
      "Epoch: 740, AUC: 0.8309, AP: 0.8079\n",
      "Epoch: 750, AUC: 0.8502, AP: 0.8258\n",
      "Epoch: 760, AUC: 0.8490, AP: 0.8171\n",
      "Epoch: 770, AUC: 0.8007, AP: 0.7768\n",
      "Epoch: 780, AUC: 0.8356, AP: 0.8113\n",
      "Epoch: 790, AUC: 0.8399, AP: 0.8063\n",
      "Epoch: 800, AUC: 0.8537, AP: 0.8229\n",
      "Epoch: 810, AUC: 0.8368, AP: 0.8075\n",
      "Epoch: 820, AUC: 0.8503, AP: 0.8182\n",
      "Epoch: 830, AUC: 0.8496, AP: 0.8170\n",
      "Epoch: 840, AUC: 0.8515, AP: 0.8244\n",
      "Epoch: 850, AUC: 0.8533, AP: 0.8240\n",
      "Epoch: 860, AUC: 0.8551, AP: 0.8267\n",
      "Epoch: 870, AUC: 0.8510, AP: 0.8232\n",
      "Epoch: 880, AUC: 0.8501, AP: 0.8154\n",
      "Epoch: 890, AUC: 0.8525, AP: 0.8206\n",
      "Epoch: 900, AUC: 0.8466, AP: 0.8188\n",
      "Epoch: 910, AUC: 0.8386, AP: 0.8140\n",
      "Epoch: 920, AUC: 0.8511, AP: 0.8225\n",
      "Epoch: 930, AUC: 0.8387, AP: 0.8106\n",
      "Epoch: 940, AUC: 0.8425, AP: 0.8128\n",
      "Epoch: 950, AUC: 0.8546, AP: 0.8266\n",
      "Epoch: 960, AUC: 0.8509, AP: 0.8233\n",
      "Epoch: 970, AUC: 0.8579, AP: 0.8270\n",
      "Epoch: 980, AUC: 0.8525, AP: 0.8269\n",
      "Epoch: 990, AUC: 0.8480, AP: 0.8189\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, 1000):\n",
    "    loss = train(epoch)\n",
    "    # AUC tells how well the model is able to distinguish real nodes from fake ones\n",
    "    # AP - Average Precision is the complementary metric of AUC \n",
    "    auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18150031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4877\n",
      "Number of edges: 20618\n",
      "Embedding sample: tensor([[ 0.2487,  0.5004,  0.2333,  ...,  0.1051,  0.2563,  0.2359],\n",
      "        [-0.0426,  0.0289,  0.5784,  ...,  0.5255, -0.4166,  0.0175],\n",
      "        [-0.0225,  0.3638,  0.3054,  ...,  0.4168, -0.0886,  0.1373],\n",
      "        ...,\n",
      "        [ 5.0085,  2.5669, -3.1004,  ...,  2.7502,  1.9506, -2.4619],\n",
      "        [ 5.0036,  2.5650, -3.0984,  ...,  2.7469,  1.9492, -2.4614],\n",
      "        [ 5.0097,  2.5674, -3.1009,  ...,  2.7510,  1.9510, -2.4620]],\n",
      "       device='cuda:0', grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "z = model.encode(x, train_data.edge_index.to(device))\n",
    "emb_sample = z[0]\n",
    "print(f'Number of nodes: {z.shape[0]}')\n",
    "print(f'Number of edges: {train_data.edge_index.shape[1]}')\n",
    "print(f'Embedding sample: {z}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".analysa (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
